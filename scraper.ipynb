{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f3c656b0-f7eb-45c8-b09e-d90226b15c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "def get_all_links(base_url):\n",
    "    \"\"\"Extract all unique links from the base URL.\"\"\"\n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = set()\n",
    "\n",
    "        # Extract all <a> tags with href attributes\n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            href = a_tag['href']\n",
    "            # Convert relative URLs to absolute URLs\n",
    "            full_url = urljoin(base_url, href)\n",
    "            # Filter links to include only those within the same domain\n",
    "            if full_url.startswith(\""):\n",
    "                links.add(full_url)\n",
    "\n",
    "        return list(links)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve links from {base_url}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "def scrape_website(url):\n",
    "    \"\"\"Scrape the headers, body paragraphs, policy, and description from a single webpage.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Remove 'related content' section\n",
    "        for related in soup.find_all('div', class_='related-topics_bg'):\n",
    "            related.decompose()  # Remove the entire element\n",
    "\n",
    "        # Remove copyright section\n",
    "        for copyright_div in soup.find_all('div', class_='master-column3'):\n",
    "            copyright_div.decompose()\n",
    "\n",
    "        # Remove Wookieepedia section\n",
    "        for wookieepedia_div in soup.find_all('div', class_='master-column2'):\n",
    "            wookieepedia_div.decompose()\n",
    "\n",
    "        # Extract header paragraphs\n",
    "        header_paragraphs = []\n",
    "        for div in soup.find_all('div', class_='textblock'):\n",
    "            inner_div = div.find('div')\n",
    "            if inner_div:\n",
    "                style = inner_div.get('style', '')\n",
    "                if 'font-size: 24px' in style:  # Adjust condition for headers\n",
    "                    text = ''.join(inner_div.stripped_strings)\n",
    "                    if text:  # Ensure it's not empty\n",
    "                        header_paragraphs.append(text)\n",
    "\n",
    "        # Extract body paragraphs\n",
    "        body_paragraphs = []\n",
    "        for div in soup.find_all('div', class_='textblock'):\n",
    "            inner_div = div.find('div')\n",
    "            if inner_div:\n",
    "                style = inner_div.get('style', '')\n",
    "                if 'font-size: 24px' not in style:  # Exclude headers\n",
    "                    text = ''.join(inner_div.stripped_strings)\n",
    "                    if text:  # Ensure it's not empty\n",
    "                        body_paragraphs.append(text)\n",
    "\n",
    "        # Extract policy (bold text)\n",
    "        policy = []\n",
    "        for div in soup.find_all('div', style=lambda value: value and 'font-weight: bold' in value):\n",
    "            span = div.find('span')\n",
    "            if span and span.text.strip():\n",
    "                policy.append(span.text.strip())\n",
    "\n",
    "        # Extract description (normal text)\n",
    "        description = []\n",
    "        for div in soup.find_all('div', style=lambda value: value and 'font-weight: normal' in value):\n",
    "            span = div.find('span')\n",
    "            if span and span.text.strip():\n",
    "                description.append(span.text.strip())\n",
    "\n",
    "        return {\n",
    "            'header_paragraphs': header_paragraphs,\n",
    "            'body_paragraphs': body_paragraphs,\n",
    "            'policy': policy,\n",
    "            'description': description\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Failed to scrape {url}. Status code: {response.status_code}\")\n",
    "        return {\n",
    "            'header_paragraphs': [],\n",
    "            'body_paragraphs': [],\n",
    "            'policy': [],\n",
    "            'description': []\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "482ed8b7-aafd-420c-9042-b178ed3a492b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "url = \""\n",
    "all_links=get_all_links(url)\n",
    "print(len(all_links))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "371c4c0f-f769-41e0-a815-e063380c8084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://siteHousing-Reports/Hotel-Revenue\n",
      "Scraping: https://siteAccess/CMS-Privileges-Key\n",
      "Scraping: https://siteRegistration-Data/ViewUpdate-History\n",
      "Scraping: https://siteSessions-Tracks-Reports/Speaker-Ratings\n",
      "Scraping: https://siteQuestion-Management/Question-Management\n",
      "Scraping: https://siteProgram-Payment/Activity-Summary\n",
      "Scraping: https://siteSessions-Tracks-Reports/Sessions-Master-Schedule\n",
      "Scraping: https://siteExpo/07-Expo-Analytics\n",
      "Scraping: https://siteHousing-Reports/Hotel-Inventory-With-RatesDay\n",
      "Scraping: https://sitePortal-Overview/Report-Types\n",
      "Scraping: https://siteTravel-Reports/07-TravelReports-Departure-Schedule-Pax\n",
      "Scraping: https://siteHousing-Reports/Hotel-Commission\n",
      "Scraping: https://siteContent-Task-Manager-Reports/Scheduled-Report\n",
      "Scraping: https://siteSessions-Tracks-Reports/Individual-Summary-Report\n",
      "Scraping: https://siteProgram-Payment/Payment-Summary\n",
      "Scraping: https://siteAdHoc/AdHoc-ReportDesigner\n",
      "Scraping: https://siteAccess/MMS-Privileges-Key\n",
      "Scraping: https://siteHousing-Reports/Room-Nights-PerBlock\n",
      "Scraping: https://siteVideo-Streaming/Video-Attendance\n",
      "Scraping: https://siteHousing-Reports/Event-Total-by-Hotel\n",
      "Scraping: https://siteProgram-Dashboard-Report/Program-Dashboard\n",
      "Scraping: https://siteHousing-Reports/Hotel-PickUp-History\n",
      "Scraping: https://siteCancellation-Reports/Overview\n",
      "Scraping: https://siteQuickList/Quick-List-Setup\n",
      "Scraping: https://siteHousing-Reports/Housing-Cancelled-List\n",
      "Scraping: https://siteRegistration-Data/Marked-Safe\n",
      "Scraping: https://siteTravel-Reports/06-TravelReports-Inbound-Schedule\n",
      "Scraping: https://siteContent-Task-Manager-Reports/Task-Report-Designer\n",
      "Scraping: https://siteAdHoc/MultiRecordEdit-ListDesigner\n",
      "Scraping: https://sitePortal-Overview/Reports-Portal-Overview\n",
      "Scraping: https://siteHousing-Reports/User-Activity-Report\n",
      "Scraping: https://siteProgram-Payment/Manual-Transactions\n",
      "Scraping: https://siteContent-Task-Management/Overview\n",
      "Scraping: https://siteRegistration-Data/Guest-Registrations\n",
      "Scraping: https://siteRegistration-Data/Status-Summary\n",
      "Scraping: https://siteSessions-Tracks-Reports/One-on-One-Meetings-Report\n",
      "Scraping: https://siteRegistration-Data/Customize-ExportedReport-HeaderFooter\n",
      "Scraping: https://siteRegistration-Data/ADA\n",
      "Scraping: https://siteVirtual-Meetings-Setup/Create-Virtual-Meeting-Reports-Portal\n",
      "Scraping: https://siteTravel-Reports/11-TravelReports-Hotel-vs-Air\n",
      "Scraping: https://siteTravel-Reports/04-TravelReports-Arrival-Schedule-Pax\n",
      "Scraping: https://siteHousing-Reports/Persons-PerBlock\n",
      "Scraping: https://siteHousing-Reports/No-Housing-Needed-List\n",
      "Scraping: https://siteRegistration-Data/Special-Requests\n",
      "Scraping: https://siteSessions-Tracks-Reports/Scheduled-Report\n",
      "Scraping: https://siteHousing-Reports/Agent-Productivity-for-Event\n",
      "Scraping: https://siteContent-Task-Manager-Reports/Task-Completion-Report\n",
      "Scraping: https://siteProgram-Payment/Credit-Card-Overpayments\n",
      "Scraping: https://siteHousing-Reports/Hotel-Payment-Summary\n",
      "Scraping: https://siteRegistration-Data/Attendance-Summary\n",
      "Scraping: https://siteTravel-Reports/02-TravelReports-Arrival-Summary\n",
      "Scraping: https://siteHousing-Reports/Master-Rooming-List\n",
      "Scraping: https://siteHousing-Reports/Housing-Change-Report\n",
      "Scraping: https://siteTravel-Reports/10-TravelReports-Complete-Itinerary\n",
      "Scraping: https://siteProgram-Payment/Credit-Card-Details\n",
      "Scraping: https://siteSearch-Records/Search-Add-Records\n",
      "Scraping: https://siteRegistration-Data/PrimaryRegistrants-WithoutGuest\n",
      "Scraping: https://siteSabre-Integration/01-Sabre-Integration\n",
      "Scraping: https://siteRegistration-Data/Primary-Guest-Registrations\n",
      "Scraping: https://siteRegistration-Data/ChangeDataReport\n",
      "Scraping: https://siteTravel-Reports/01-TravelReports-Overview\n",
      "Scraping: https://siteSessions-Tracks-Reports/Waitlist-Report\n",
      "Scraping: https://siteInvitee-Reports/Overview\n",
      "Scraping: https://siteRegistration-Data/Comment-Summary\n",
      "Scraping: https://siteProgram-Payment/Program-Fee-Details\n",
      "Scraping: https://siteHousing-Reports/Agent-Productivity-for-Company\n",
      "Scraping: https://siteProgram-Payment/Payment-Transaction-Details\n",
      "Scraping: https://siteProgram-Payment/Program-Fee-No-Selections\n",
      "Scraping: https://siteHousing-Reports/Gaps-in-Stays\n",
      "Scraping: https://siteRegistration-Data/Document-Download\n",
      "Scraping: https://siteHousing-Reports/Pending-Room-Requests-All\n",
      "Scraping: https://siteTravel-Reports/12-TravelReports-AirUpload\n",
      "Scraping: https://siteTravel-Reports/05-TravelReports-Arrival-Schedule-Date\n",
      "Scraping: https://siteAccess/Reports-Portal-Access\n",
      "Scraping: https://siteTravel-Reports/09-TravelReports-Outbound-Schedule\n",
      "Scraping: https://siteTravel-Reports/08-TravelReports-Departure-Schedule-Date\n",
      "Scraping: https://siteProgram-Payment/Activity-No-Selections\n",
      "Scraping: https://siteTravel-Reports/03-TravelReports-Departure-Summary\n",
      "Scraping: https://siteExpo/06-Expo-Setup-EQL\n",
      "Scraping: https://siteContent-Task-Manager-Reports/Overview\n",
      "Scraping: https://siteAdHoc/AdHoc-Reports-SendEmail\n",
      "Scraping: https://siteAdHoc/AdHoc-Reports-ScheduleReport\n",
      "Scraping: https://siteProgram-Payment/Activity-Details\n",
      "Scraping: https://siteSessions-Tracks-Reports/Session-Reports-Details\n",
      "Scraping: https://siteHousing-Reports/Housing-Overview\n"
     ]
    }
   ],
   "source": [
    "def scrape_all_subpages(base_url):\n",
    "    \"\"\"Scrape all subpages of the given website.\"\"\"\n",
    "    # Get all subpage links\n",
    "    links = get_all_links(base_url)\n",
    "    all_content = {}\n",
    "\n",
    "    for link in links:\n",
    "        print(f\"Scraping: {link}\")\n",
    "        page_content = scrape_website(link)\n",
    "        if page_content:\n",
    "            # Extract the last part of the URL to use as the dictionary key\n",
    "            last_part = link.rstrip('/').split('/')[-1]  # Get the last segment of the URL\n",
    "            page_content = scrape_website(link)\n",
    "            # Add the scraped content to the dictionary with the last part of the URL as the key\n",
    "            all_content[last_part] = page_content\n",
    "\n",
    "    return all_content\n",
    "web_content=scrape_all_subpages(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
