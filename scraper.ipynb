{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3c656b0-f7eb-45c8-b09e-d90226b15c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "def get_all_links(base_url, start):\n",
    "    \"\"\"Extract all unique links from the base URL.\"\"\"\n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = set()\n",
    "\n",
    "        # Extract all <a> tags with href attributes\n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            href = a_tag['href']\n",
    "            # Convert relative URLs to absolute URLs\n",
    "            full_url = urljoin(base_url, href)\n",
    "            # Filter links to include only those within the same domain\n",
    "            if full_url.startswith(start):\n",
    "                links.add(full_url)\n",
    "\n",
    "        return list(links)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve links from {base_url}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "def scrape_website(url):\n",
    "    \"\"\"Scrape the headers, body paragraphs, policy, and description from a single webpage.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Remove 'related content' section\n",
    "        for related in soup.find_all('div', class_='related-topics_bg'):\n",
    "            related.decompose()  # Remove the entire element\n",
    "\n",
    "        # Remove copyright section\n",
    "        for copyright_div in soup.find_all('div', class_='master-column3'):\n",
    "            copyright_div.decompose()\n",
    "\n",
    "        # Remove Wookieepedia section\n",
    "        for wookieepedia_div in soup.find_all('div', class_='master-column2'):\n",
    "            wookieepedia_div.decompose()\n",
    "\n",
    "        # Extract header paragraphs\n",
    "        header_paragraphs = []\n",
    "        for div in soup.find_all('div', class_='textblock'):\n",
    "            inner_div = div.find('div')\n",
    "            if inner_div:\n",
    "                style = inner_div.get('style', '')\n",
    "                if 'font-size: 24px' in style:  # Adjust condition for headers\n",
    "                    text = ''.join(inner_div.stripped_strings)\n",
    "                    if text:  # Ensure it's not empty\n",
    "                        header_paragraphs.append(text)\n",
    "\n",
    "        # Extract body paragraphs\n",
    "        body_paragraphs = []\n",
    "        for div in soup.find_all('div', class_='textblock'):\n",
    "            inner_div = div.find('div')\n",
    "            if inner_div:\n",
    "                style = inner_div.get('style', '')\n",
    "                if 'font-size: 24px' not in style:  # Exclude headers\n",
    "                    text = ''.join(inner_div.stripped_strings)\n",
    "                    if text:  # Ensure it's not empty\n",
    "                        body_paragraphs.append(text)\n",
    "\n",
    "        # Extract policy (bold text)\n",
    "        policy = []\n",
    "        for div in soup.find_all('div', style=lambda value: value and 'font-weight: bold' in value):\n",
    "            span = div.find('span')\n",
    "            if span and span.text.strip():\n",
    "                policy.append(span.text.strip())\n",
    "\n",
    "        # Extract description (normal text)\n",
    "        description = []\n",
    "        for div in soup.find_all('div', style=lambda value: value and 'font-weight: normal' in value):\n",
    "            span = div.find('span')\n",
    "            if span and span.text.strip():\n",
    "                description.append(span.text.strip())\n",
    "\n",
    "        return {\n",
    "            'header_paragraphs': header_paragraphs,\n",
    "            'body_paragraphs': body_paragraphs,\n",
    "            'policy': policy,\n",
    "            'description': description\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Failed to scrape {url}. Status code: {response.status_code}\")\n",
    "        return {\n",
    "            'header_paragraphs': [],\n",
    "            'body_paragraphs': [],\n",
    "            'policy': [],\n",
    "            'description': []\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "482ed8b7-aafd-420c-9042-b178ed3a492b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "url = \"https://en.wikipedia.org/wiki/Example\"\n",
    "all_links=get_all_links(url, \"https://en.wikipedia.org/wiki/Example\")\n",
    "print(len(all_links))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "371c4c0f-f769-41e0-a815-e063380c8084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://en.wikipedia.org/wiki/Example#See_also\n",
      "Scraping: https://en.wikipedia.org/wiki/Example#bodyContent\n",
      "Scraping: https://en.wikipedia.org/wiki/Example.com\n",
      "Scraping: https://en.wikipedia.org/wiki/Example#Arts\n",
      "Scraping: https://en.wikipedia.org/wiki/Example\n",
      "Scraping: https://en.wikipedia.org/wiki/Example_(musician)\n",
      "Scraping: https://en.wikipedia.org/wiki/Example_(album)\n"
     ]
    }
   ],
   "source": [
    "def scrape_all_subpages(base_url):\n",
    "    \"\"\"Scrape all subpages of the given website.\"\"\"\n",
    "    # Get all subpage links\n",
    "    links = get_all_links(base_url, \"https://en.wikipedia.org/wiki/Example\")\n",
    "    all_content = {}\n",
    "\n",
    "    for link in links:\n",
    "        print(f\"Scraping: {link}\")\n",
    "        page_content = scrape_website(link)\n",
    "        if page_content:\n",
    "            # Extract the last part of the URL to use as the dictionary key\n",
    "            last_part = link.rstrip('/').split('/')[-1]  # Get the last segment of the URL\n",
    "            page_content = scrape_website(link)\n",
    "            # Add the scraped content to the dictionary with the last part of the URL as the key\n",
    "            all_content[last_part] = page_content\n",
    "\n",
    "    return all_content\n",
    "web_content=scrape_all_subpages(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
